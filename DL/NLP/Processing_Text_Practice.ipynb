{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing and Wrangling**\n",
        "\n",
        "Text wrangling (also called preprocessing or normalization) is a process that consists of\n",
        "a series of steps to wrangle, clean, and standardize textual data into a form that could be\n",
        "consumed by other NLP and intelligent systems powered by machine learning and deep\n",
        "learning. Common techniques for preprocessing include cleaning text, tokenizing text,\n",
        "removing special characters, case conversion, correcting spellings, removing stopwords\n",
        "and other unnecessary terms, stemming, and lemmatization.\n",
        "\n",
        "The key idea is to remove unnecessary content\n",
        "from one or more text documents in a corpus (or corpora) and get clean text documents. The following list gives us an idea of some of the most popular text preprocessing and understanding techniques, which are:\n",
        "\n",
        "*   Removing HTML tags\n",
        "* Tokenization\n",
        "* Removing unnecessary tokens and stopwords\n",
        "* Handling contractions\n",
        "* Correcting spelling errors\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "* Tagging\n",
        "* Chunking\n",
        "* Parsing\n"
      ],
      "metadata": {
        "id": "sJdv-1nNFCBM"
      },
      "id": "sJdv-1nNFCBM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Removing HTML Tags***\n",
        "\n",
        "Often, unstructured text contains a lot of noise, especially if you use techniques\n",
        "like web scraping or screen scraping to retrieve data from web pages, blogs, and\n",
        "online repositories. HTML tags, JavaScript, and Iframe tags typically don’t add much\n",
        "value to understanding and analyzing text. Our main intent is to extract meaningful\n",
        "textual content from the data extracted from the web."
      ],
      "metadata": {
        "id": "iKTwfEnKF0rQ"
      },
      "id": "iKTwfEnKF0rQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.content\n",
        "print(content[1163:2200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-04N9mHGIb4",
        "outputId": "17ee4e8e-e32e-4ff9-a363-3a0b9d540921"
      },
      "id": "u-04N9mHGIb4",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'gin-top: 0;\\r\\n    margin-bottom: 0;\\r\\n}\\r\\n#pg-header #pg-machine-header strong {\\r\\n    font-weight: normal;\\r\\n}\\r\\n#pg-header #pg-start-separator, #pg-footer #pg-end-separator {\\r\\n    margin-bottom: 3em;\\r\\n    margin-left: 0;\\r\\n    margin-right: auto;\\r\\n    margin-top: 2em;\\r\\n    text-align: center\\r\\n}\\r\\n\\r\\n    .xhtml_center {text-align: center; display: block;}\\r\\n    .xhtml_center table {\\r\\n        display: table;\\r\\n        text-align: left;\\r\\n        margin-left: auto;\\r\\n        margin-right: auto;\\r\\n        }</style><title>The Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis, by Anonymous</title><style>/* ************************************************************************\\r\\n * classless css copied from https://www.pgdp.net/wiki/CSS_Cookbook/Styles\\r\\n * ********************************************************************** */\\r\\n/* ************************************************************************\\r\\n * set the body margins to allow whitespace along sides of window\\r\\n * ******************************************'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text"
      ],
      "metadata": {
        "id": "5TFWn_k2GTVv"
      },
      "id": "5TFWn_k2GTVv",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1163:2045])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt86X_EBGbge",
        "outputId": "ca2d6f3d-d0b7-46cb-d38b-38f5781595b2"
      },
      "id": "Rt86X_EBGbge",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oid; and darkness was\n",
            "           upon the face of the deep. And the Spirit of God moved upon\n",
            "           the face of the waters.\n",
            "01:001:003 And God said, Let there be light: and there was light.\n",
            "01:001:004 And God saw the light, that it was good: and God divided the\n",
            "           light from the darkness.\n",
            "01:001:005 And God called the light Day, and the darkness he called\n",
            "           Night. And the evening and the morning were the first day.\n",
            "01:001:006 And God said, Let there be a firmament in the midst of the\n",
            "           waters, and let it divide the waters from the waters.\n",
            "01:001:007 And God made the firmament, and divided the waters which were\n",
            "           under the firmament from the waters which were above the\n",
            "           firmament: and it was so.\n",
            "01:001:008 And God called the firmament Heaven. And the evening and the\n",
            "           morning were the second day.\n",
            "01:001:009 And Go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2b7c06",
      "metadata": {
        "id": "6d2b7c06"
      },
      "source": [
        "# ***Text Tokenization***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokens are\n",
        "independent and minimal textual components that have some definite syntax and\n",
        "semantics. A paragraph of text or a text document has several components, including\n",
        "sentences, which can be further broken down into clauses, phrases, and words. The\n",
        "most popular tokenization techniques include sentence and word tokenization, which\n",
        "are used to break down a text document (or corpus) into sentences and each sentence\n",
        "into words. Thus, tokenization can be defined as the process of breaking down or\n",
        "splitting textual data into smaller and more meaningful components called tokens. In the\n",
        "following sections, we look at some ways to tokenize text into sentences and words."
      ],
      "metadata": {
        "id": "fvoFAUtt2nyZ"
      },
      "id": "fvoFAUtt2nyZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Tokenization**\n",
        "\n",
        "\n",
        "Sentence tokenization is the process of splitting a text corpus into sentences that act\n",
        "as the first level of tokens the corpus is comprised of. This is also known as sentence\n",
        "segmentation, since we try to segment the text into meaningful sentences. Any text\n",
        "corpus is a body of text where each paragraph comprises several sentences. There are\n",
        "various ways to perform sentence tokenization. Basic techniques include looking for\n",
        "specific delimiters between sentences like a period (.) or a newline character (\\n) and\n",
        "sometimes even a semicolon (;). We will use the NLTK framework, which provides\n",
        "various interfaces for performing sentence tokenization. We primarily focus on the\n",
        "following sentence tokenizers:\n",
        "* sent_tokenize\n",
        "* Pretrained sentence tokenization models\n",
        "* PunktSentenceTokenizer\n",
        "* RegexpTokenizer"
      ],
      "metadata": {
        "id": "PSecJlXr29Jp"
      },
      "id": "PSecJlXr29Jp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can tokenize sentences, we need some text on which we can try these\n",
        "operations. We load some sample text and part of the Gutenberg corpus available in\n",
        "NLTK. We load the necessary dependencies using the following snippet."
      ],
      "metadata": {
        "id": "j4qPGY1l3MV5"
      },
      "id": "j4qPGY1l3MV5"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pIf3JzPC1fA5"
      },
      "id": "pIf3JzPC1fA5",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyCrcXx39EGl",
        "outputId": "b0ffbecf-df64-4674-9528-2977706697d2"
      },
      "id": "RyCrcXx39EGl",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1a59a12a",
      "metadata": {
        "id": "1a59a12a"
      },
      "outputs": [],
      "source": [
        "# loading text corpora\n",
        "alice = gutenberg.raw(fileids='carroll-alice.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\"\"\""
      ],
      "metadata": {
        "id": "rVS6obDY3g6x"
      },
      "id": "rVS6obDY3g6x",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TCTZRENK4Efh",
        "outputId": "4bd3efd9-66f7-4342-afeb-6bcb390c81a5"
      },
      "id": "TCTZRENK4Efh",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the length of the “Alice in Wonderland” corpus and the first few lines in\n",
        "it using the following snippet."
      ],
      "metadata": {
        "id": "YYDk7LH34pGw"
      },
      "id": "YYDk7LH34pGw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Total characters in Alice in Wonderland\n",
        "len(alice)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yXMdYmI4oFe",
        "outputId": "b56469b9-0b20-4a52-919b-f4aa9c8856b5"
      },
      "id": "_yXMdYmI4oFe",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144395"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First 100 characters in the corpus\n",
        "alice[0:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xRbE36Fm59ow",
        "outputId": "6bbde236-0ff1-4058-eb68-ede2d0c18cc2"
      },
      "id": "xRbE36Fm59ow",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Default Sentence Tokenizer**\n",
        "\n",
        "The nltk.sent_tokenize(...) function is the default sentence tokenization function\n",
        "that NLTK recommends and it uses an instance of the PunktSentenceTokenizer class\n",
        "internally. However, this is not just a normal object or instance of that class. It has been\n",
        "pretrained on several language models and works really well on many popular languages\n",
        "besides English. The following snippet shows the basic usage of this function on our text\n",
        "samples"
      ],
      "metadata": {
        "id": "5XHaqNKH41zJ"
      },
      "id": "5XHaqNKH41zJ"
    },
    {
      "cell_type": "code",
      "source": [
        "default_st = nltk.sent_tokenize\n",
        "nltk.download('punkt')\n",
        "alice_sentences = default_st(text=alice)\n",
        "sample_sentences = default_st(text=sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSlkHTsj9lUW",
        "outputId": "c92b17a9-654d-4076-9f3b-8be06b93216a"
      },
      "id": "hSlkHTsj9lUW",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total sentences in sample_text:', len(sample_sentences))\n",
        "print('Sample text sentences :-')\n",
        "print(np.array(sample_sentences))\n",
        "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
        "print('First 5 sentences in alice:-')\n",
        "print(np.array(alice_sentences[0:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U4usJ_D93xJ",
        "outputId": "ec6f7e62-3bd3-48ba-a269-ec3542cfcae0"
      },
      "id": "2U4usJ_D93xJ",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences in sample_text: 4\n",
            "Sample text sentences :-\n",
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
            "\n",
            "Total sentences in alice: 1625\n",
            "First 5 sentences in alice:-\n",
            "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
            " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
            " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
            " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
            " 'Oh dear!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained Sentence Tokenizer Models**\n",
        "\n",
        "Suppose we were dealing with German text. We can use sent_tokenize, which\n",
        "is already trained, or load a pretrained tokenization model on German text into a\n",
        "PunktSentenceTokenizer instance and perform the same operation. The following\n",
        "snippet shows this. We start by loading a German text corpus and inspecting it\n"
      ],
      "metadata": {
        "id": "SgiodvBb-uCu"
      },
      "id": "SgiodvBb-uCu"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import europarl_raw\n",
        "nltk.download('europarl_raw')\n",
        "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
        "# Total characters in the corpus\n",
        "print(len(german_text))\n",
        "# First 100 characters in the corpus\n",
        "print(german_text[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRvfmRRk-zfe",
        "outputId": "2fd64907-a488-4256-af11-ec63107bd740"
      },
      "id": "KRvfmRRk-zfe",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157171\n",
            " \n",
            "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# default sentence tokenizer\n",
        "german_sentences_def = default_st(text=german_text, language='german')"
      ],
      "metadata": {
        "id": "9A8MeY0D_BRd"
      },
      "id": "9A8MeY0D_BRd",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
        "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
        "german_sentences = german_tokenizer.tokenize(german_text)"
      ],
      "metadata": {
        "id": "-RVkJLhq_EO4"
      },
      "id": "-RVkJLhq_EO4",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify the type of german_tokenizer\n",
        "# should be PunktSentenceTokenizer\n",
        "print(type(german_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNAOhZP3_LUx",
        "outputId": "91fb1272-2d3b-4e38-dce6-39e8968b9896"
      },
      "id": "CNAOhZP3_LUx",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if results of both tokenizers match\n",
        "# should be True\n",
        "print(german_sentences_def == german_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLRs1JNr_QQh",
        "outputId": "c67e61f5-217e-4dbd-f3d0-49566be4f756"
      },
      "id": "pLRs1JNr_QQh",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PunktSentenceTokenizer**\n",
        "Using the default PunktSentenceTokenizer class is also pretty straightforward, as the\n",
        "following snippet shows.\n"
      ],
      "metadata": {
        "id": "E5Tp8-mfHBop"
      },
      "id": "E5Tp8-mfHBop"
    },
    {
      "cell_type": "code",
      "source": [
        "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
        "sample_sentences = punkt_st.tokenize(sample_text)\n",
        "print(np.array(sample_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgRUuF4GHNNG",
        "outputId": "ea85a1b0-5bfd-4ac6-fa09-4318aa0c1ec4"
      },
      "id": "qgRUuF4GHNNG",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpTokenizer**\n",
        "\n",
        "\n",
        "The last tokenizer we cover in sentence tokenization is using an instance of the\n",
        "RegexpTokenizer class to tokenize text into sentences, where we will use specific regular\n",
        "expression-based patterns to segment sentences. Recall the regular expressions from the\n",
        "previous chapter if you want to refresh your memory. The following snippet shows how\n",
        "to use a regex pattern to tokenize sentences"
      ],
      "metadata": {
        "id": "QL8U_KNqHXJp"
      },
      "id": "QL8U_KNqHXJp"
    },
    {
      "cell_type": "code",
      "source": [
        "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,gaps=True)\n",
        "sample_sentences = regex_st.tokenize(sample_text)\n",
        "print(np.array(sample_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCRsFtzqHaEK",
        "outputId": "9e621783-d07e-40b6-b9de-aac74e79a78f"
      },
      "id": "nCRsFtzqHaEK",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization**\n",
        "\n",
        "\n",
        "Word tokenization is the process of splitting or segmenting sentences into their\n",
        "constituent words. A sentence is a collection of words and with tokenization we\n",
        "essentially split a sentence into a list of words that can be used to reconstruct the\n",
        "sentence. Word tokenization is really important in many processes, especially in\n",
        "cleaning and normalizing text where operations like stemming and lemmatization work\n",
        "on each individual word based on its respective stems and lemma. Similar to sentence\n",
        "tokenization, NLTK provides various useful interfaces for word tokenization. We will\n",
        "touch up on the following main interfaces:\n",
        "* word_tokenize\n",
        "* TreebankWordTokenizer\n",
        "* TokTokTokenizer\n",
        "* RegexpTokenizer\n",
        "* Inherited tokenizers from RegexpTokenizer"
      ],
      "metadata": {
        "id": "FcI1xW8l_w5k"
      },
      "id": "FcI1xW8l_w5k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Default Word Tokenizer**\n",
        "\n",
        "The nltk.word_tokenize(...) function is the default and recommended word\n",
        "tokenizer, as specified by NLTK. This tokenizer is an instance or object of the\n",
        "TreebankWordTokenizer class in its internal implementation and acts as a wrapper to\n",
        "that core class. The following snippet illustrates its usage."
      ],
      "metadata": {
        "id": "Mbb2zwbnHtA8"
      },
      "id": "Mbb2zwbnHtA8"
    },
    {
      "cell_type": "code",
      "source": [
        "default_wt = nltk.word_tokenize\n",
        "words = default_wt(sample_text)\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2zMIEr7_rUK",
        "outputId": "7a773e5e-d198-4baf-c2c7-730cec58505d"
      },
      "id": "C2zMIEr7_rUK",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TreebankWordTokenizer**\n",
        "\n",
        "The TreebankWordTokenizer is based on the Penn Treebank and uses various regular\n",
        "expressions to tokenize the text. Of course, one primary assumption here is that we have\n",
        "already performed sentence tokenization beforehand. Some of the main features of this tokenizer are mentioned here:\n",
        "* Splits and separates out periods that appear at the end of a sentence\n",
        "* Splits and separates commas and single quotes when followed by whitespace\n",
        "* Most punctuation characters are split and separated intobindependent tokens\n",
        "* Splits words with standard contractions, such as don’t to do and n’t"
      ],
      "metadata": {
        "id": "3n0luaQG_8bQ"
      },
      "id": "3n0luaQG_8bQ"
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_wt = nltk.TreebankWordTokenizer()\n",
        "words = treebank_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-8zawJ_4YR",
        "outputId": "dfb10476-18d6-40f8-97dd-6b09d91af341"
      },
      "id": "h0-8zawJ_4YR",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TokTokTokenizer**\n",
        "\n",
        "\n",
        "TokTokTokenizer is one of the newer tokenizers introduced by NLTK present in the\n",
        "nltk.tokenize.toktok module. In general, the tok-tok tokenizer is a general tokenizer,\n",
        "where it assumes that the input has one sentence per line. Hence, only the final period\n",
        "is tokenized. However, as needed, we can remove the other periods from the words\n",
        "using regular expressions. Tok-tok has been tested on, and gives reasonably good results\n",
        "for, English, Persian, Russian, Czech, French, German, Vietnamese, and many other\n",
        "languages."
      ],
      "metadata": {
        "id": "FmbQmZPDAo98"
      },
      "id": "FmbQmZPDAo98"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "words = tokenizer.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH-msZk4ARWk",
        "outputId": "4b07dabd-abf7-45cb-cced-ce68496d9382"
      },
      "id": "XH-msZk4ARWk",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
              "       'the', 'previous', 'record-holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpTokenizer**\n",
        "\n",
        "\n",
        "We now look at how to use regular expressions and the RegexpTokenizer class to\n",
        "tokenize sentences into words. Remember that there are two main parameters that\n",
        "are useful in tokenization—the regex pattern for building the tokenizer and the gaps\n",
        "parameter, which, if set to true, is used to find the gaps between the tokens. Otherwise, it\n",
        "is used to find the tokens themselves."
      ],
      "metadata": {
        "id": "bXvsv0DkAtKI"
      },
      "id": "bXvsv0DkAtKI"
    },
    {
      "cell_type": "code",
      "source": [
        "# pattern to identify tokens themselves\n",
        "TOKEN_PATTERN = r'\\w+'\n",
        "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,gaps=False)\n",
        "words = regex_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szhhvBcRAwlJ",
        "outputId": "0833d8a7-bccd-424f-9aee-ce94958e84d5"
      },
      "id": "szhhvBcRAwlJ",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', 's', 'most', 'powerful', 'supercomputer',\n",
              "       'beats', 'China', 'The', 'US', 'has', 'unveiled', 'the', 'world',\n",
              "       's', 'most', 'powerful', 'supercomputer', 'called', 'Summit',\n",
              "       'beating', 'the', 'previous', 'record', 'holder', 'China', 's',\n",
              "       'Sunway', 'TaihuLight', 'With', 'a', 'peak', 'performance', 'of',\n",
              "       '200', '000', 'trillion', 'calculations', 'per', 'second', 'it',\n",
              "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
              "       'which', 'is', 'capable', 'of', '93', '000', 'trillion',\n",
              "       'calculations', 'per', 'second', 'Summit', 'has', '4', '608',\n",
              "       'servers', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inherited Tokenizers from RegexpTokenizer**\n",
        "\n",
        "Besides the base RegexpTokenizer class, there are several derived classes that\n",
        "perform different types of word tokenization. The WordPunktTokenizer uses the pattern\n",
        "r'\\w+|[^\\w\\s]+' to tokenize sentences into independent alphabetic and\n",
        "non-alphabetic tokens."
      ],
      "metadata": {
        "id": "BC2ZTwUIJQaq"
      },
      "id": "BC2ZTwUIJQaq"
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
        "words = wordpunkt_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F0I95crJVJY",
        "outputId": "28c5356d-5366-4064-f32c-7a4dfff33a4a"
      },
      "id": "9F0I95crJVJY",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"',\", 'beating', 'the',\n",
              "       'previous', 'record', '-', 'holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200',\n",
              "       ',', '000', 'trillion', 'calculations', 'per', 'second', ',', 'it',\n",
              "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
              "       ',', 'which', 'is', 'capable', 'of', '93', ',', '000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4', ',',\n",
              "       '608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WhitespaceTokenizer tokenizes sentences into words based on whitespace, like\n",
        "tabs, newlines, and spaces. The following snippet shows demonstrations of these tokenizers."
      ],
      "metadata": {
        "id": "XGuDjb7mJcfy"
      },
      "id": "XGuDjb7mJcfy"
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_wt = nltk.WhitespaceTokenizer()\n",
        "words = whitespace_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnaKpb9kJdRw",
        "outputId": "29264a99-35f2-4666-fa3a-dbc3b93a83a7"
      },
      "id": "nnaKpb9kJdRw",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
              "       'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the',\n",
              "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
              "       \"'Summit',\", 'beating', 'the', 'previous', 'record-holder',\n",
              "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
              "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
              "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
              "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
              "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Building Robust Tokenizers***"
      ],
      "metadata": {
        "id": "NDM27y-cA6O4"
      },
      "id": "NDM27y-cA6O4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a typical NLP pipeline, I recommend leveraging state-of-the-art libraries like NLTK and using some of their robust utilities to build a custom function to perform\n",
        "both sentence- and word-level tokenization. A simple example is depicted in the\n",
        "following snippets. We start with looking at how we can leverage NLTK."
      ],
      "metadata": {
        "id": "MiRG76InJjmG"
      },
      "id": "MiRG76InJjmG"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens"
      ],
      "metadata": {
        "id": "QzIo7TwiA9BA"
      },
      "id": "QzIo7TwiA9BA",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = tokenize_text(sample_text)\n",
        "np.array(sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqPIOZ0UBGWx",
        "outputId": "3d496be1-121c-4800-8e22-af3a8732d69c"
      },
      "id": "TqPIOZ0UBGWx",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-1723b8566941>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.array(sents)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
              "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Removing Accented Characters***\n",
        "\n",
        "Usually in any text corpus, you might be dealing with accented characters/letters, especially\n",
        "if you only want to analyze the English language. Hence, we need to make sure that these\n",
        "characters are converted and standardized into ASCII characters. This shows a simple\n",
        "example—converting é to e. The following function is a simple way of tackling this task."
      ],
      "metadata": {
        "id": "OeUPnAO9EdEV"
      },
      "id": "OeUPnAO9EdEV"
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata"
      ],
      "metadata": {
        "id": "gAEAhD88EiW0"
      },
      "id": "gAEAhD88EiW0",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text"
      ],
      "metadata": {
        "id": "UGY7kAyBEtaE"
      },
      "id": "UGY7kAyBEtaE",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MOArqaJ_FOrL",
        "outputId": "7a2037c3-5598-409d-e836-fa99fe023089"
      },
      "id": "MOArqaJ_FOrL",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Expanding Contractions***\n",
        "\n",
        "Contractions are shortened versions of words or syllables. These exist in written and\n",
        "spoken forms. Shortened versions of existing words are created by removing specific\n",
        "letters and sounds. In the case of English contractions, they are often created by\n",
        "removing one of the vowels from the word. Examples include “is not” to “isn’t” and\n",
        "“will not” to “won’t”, where you can notice the apostrophe being used to denote the\n",
        "contraction and some of the vowels and other letters being removed.\n"
      ],
      "metadata": {
        "id": "BUe9Z4KlBjDF"
      },
      "id": "BUe9Z4KlBjDF"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb3wYomCCPWr",
        "outputId": "5db9ee75-2152-43d3-9689-4ca7e9bc1bdc"
      },
      "id": "cb3wYomCCPWr",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import contractions\n",
        "from contractions import contractions_dict\n",
        "import re"
      ],
      "metadata": {
        "id": "94TTrdOQBWcS"
      },
      "id": "94TTrdOQBWcS",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
        "  contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
        "\n",
        "  def expand_match(contraction):\n",
        "    match = contraction.group(0)\n",
        "    first_char = match[0]\n",
        "    expanded_contraction = contraction_mapping.get(match)\\\n",
        "                          if contraction_mapping.get(match)\\\n",
        "                          else contraction_mapping.get(match.lower())\n",
        "    expanded_contraction = first_char+expanded_contraction[1:]\n",
        "    return expanded_contraction\n",
        "  expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "  expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "  return expanded_text"
      ],
      "metadata": {
        "id": "8zBh9oRAB0go"
      },
      "id": "8zBh9oRAB0go",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expand_contractions(\"Y'all can't expand contractions I'd think\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "POfwS2v3DEF_",
        "outputId": "1e04a261-957e-4cfb-9c37-622b2cfb7905"
      },
      "id": "POfwS2v3DEF_",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You all cannot expand contractions I would think'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Removing Special Characters***\n",
        "\n",
        "Special characters and symbols are usually non-alphanumeric characters or even\n",
        "occasionally numeric characters (depending on the problem), which add to the extra\n",
        "noise in unstructured text. Usually, simple regular expressions (regexes) can be used to\n",
        "remove them. The following code helps us remove special characters"
      ],
      "metadata": {
        "id": "V9dvReAVDJXB"
      },
      "id": "V9dvReAVDJXB"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "  pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "pzldH32VDGxB"
      },
      "id": "pzldH32VDGxB",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", remove_digits=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FWQ-rRqJDUuk",
        "outputId": "6285f24b-01f6-4c47-b5f6-b9ba717bce7c"
      },
      "id": "FWQ-rRqJDUuk",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun What do you think '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Case Conversions***\n",
        "\n",
        "Often you might want to modify the case of words or sentences to make things easier,\n",
        "like matching specific words or tokens. Usually, there are two types of case conversion\n",
        "operations that are used a lot. These are lower- and uppercase conversions, where a\n",
        "body of text is converted completely to lowercase or uppercase. There are other forms\n",
        "also like sentence case or title case. Lowercase is a form where all the letters of the text\n",
        "are small letters and in uppercase they are all capitalized. Title case will capitalize the\n",
        "first letter of each word in the sentence."
      ],
      "metadata": {
        "id": "HJvhqJxHDcGX"
      },
      "id": "HJvhqJxHDcGX"
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercase\n",
        "text = 'The quick brown fox jumped over The Big Dog'\n",
        "text.lower()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XIJJN6TKDYIP",
        "outputId": "3774b3df-3db3-4e8e-f20a-51f74ee52328"
      },
      "id": "XIJJN6TKDYIP",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the quick brown fox jumped over the big dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uppercase\n",
        "text.upper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "j-pcjuCuDn_i",
        "outputId": "59b1ab38-9b35-4003-de33-260e7046d430"
      },
      "id": "j-pcjuCuDn_i",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'THE QUICK BROWN FOX JUMPED OVER THE BIG DOG'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# title case\n",
        "text.title()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Bt8D8tTwDuad",
        "outputId": "4d366ccb-13b6-4327-9afb-2a79644f1367"
      },
      "id": "Bt8D8tTwDuad",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Quick Brown Fox Jumped Over The Big Dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Text Correction***\n",
        "\n",
        "One of the main challenges faced in text wrangling is the presence of incorrect words\n",
        "in the text. The definition of incorrect here covers words that have spelling mistakes as\n",
        "well as words with several letters repeated that do not contribute much to its overall\n",
        "significance. To illustrate some examples, the word “finally” could be mistakenly written\n",
        "as “fianlly” or someone expressing intense emotion could write it as “finalllllyyyyyy”."
      ],
      "metadata": {
        "id": "1quXK4oUMszR"
      },
      "id": "1quXK4oUMszR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correcting Repeating Characters**\n",
        "\n",
        "We just mentioned words that often contain several repeating characters that could be\n",
        "due to incorrect spellings, slang language, or even people wanting to express strong\n",
        "emotions. We show a method here that uses a combination of syntax and semantics to\n",
        "correct these words. We start by correcting the syntax of these words and then move on\n",
        "to semantics.\n",
        "\n",
        "We will now utilize the WordNet\n",
        "corpus to check for valid words at each stage and terminate the loop once it is obtained.\n",
        "This introduces the semantic correction needed for our algorithm, as illustrated in the\n",
        "following snippet.\n"
      ],
      "metadata": {
        "id": "cxqmyiheD2J4"
      },
      "id": "cxqmyiheD2J4"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRILJL0A9dtw",
        "outputId": "84de1351-4f41-4458-84bb-63720fb899d1"
      },
      "id": "NRILJL0A9dtw",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "def remove_repeated_characters(tokens):\n",
        "  repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "  match_substitution = r'\\1\\2\\3'\n",
        "\n",
        "  def replace(old_word):\n",
        "    if wordnet.synsets(old_word):\n",
        "      return old_word\n",
        "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "    return replace(new_word) if new_word != old_word else new_word\n",
        "  correct_tokens = [replace(word) for word in tokens]\n",
        "  return correct_tokens"
      ],
      "metadata": {
        "id": "DKqETVoyDvoC"
      },
      "id": "DKqETVoyDvoC",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this snippet, we use the inner function replace() to basically emulate the\n",
        "behavior of our algorithm that we illustrated earlier and then call it repeatedly on each\n",
        "token in a sentence in the outer function remove_repeated_characters()"
      ],
      "metadata": {
        "id": "xZwAP8c8Lv-p"
      },
      "id": "xZwAP8c8Lv-p"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = 'GLA is realllllyyy amaaazingggg'\n",
        "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
        "' '.join(correct_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vZbWQ2I5LrDU",
        "outputId": "9fa126bb-36b9-4c6a-a15d-ff5d85e424e1"
      },
      "id": "vZbWQ2I5LrDU",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GLA is really amazing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correcting Spellings**\n",
        "The second problem we face with words is incorrect or wrong spellings that occur due to\n",
        "human error and even machine based errors, which you might have seen with features\n",
        "like auto-correcting text. There are various ways to deal with incorrect spellings where\n",
        "the final objective is to have tokens of text with the correct spelling."
      ],
      "metadata": {
        "id": "fBU9-NkUM_gj"
      },
      "id": "fBU9-NkUM_gj"
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "w = Word('fianlly')\n",
        "w.correct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XdvQgZDGL_02",
        "outputId": "79e1b90b-7868-4d94-a7cc-a6abe5056697"
      },
      "id": "XdvQgZDGL_02",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'finally'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check suggestions\n",
        "w.spellcheck()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvSJ0IdvNzC4",
        "outputId": "5dc69da3-02d3-4c2d-fe0b-f959899d5a67"
      },
      "id": "XvSJ0IdvNzC4",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('finally', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another example\n",
        "w = Word('flaot')\n",
        "w.spellcheck()"
      ],
      "metadata": {
        "id": "OspTZvKiN7S7",
        "outputId": "63267fa9-d738-404b-bfff-30c0086cf4e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OspTZvKiN7S7",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('flat', 0.85), ('float', 0.15)]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Stemming***\n",
        "To understand the process of stemming, we need to understand what word stems\n",
        "represent. We talked about morphemes, which are the smallest independent unit in any natural language. Morphemes consist of units that are stems and affixes. Affixes are units like prefixes, suffixes, and so on, which are attached to word stems to change their meaning or create a new word altogether. Word stems are also\n",
        "often known as the base form of a word and we can create new words by attaching affixes\n",
        "to them. This process is known as inflection. The reverse of this is obtaining the base\n",
        "form of a word from its inflected form and this is known as stemming.\n",
        "\n",
        "Consider the word “JUMP”, you can add affixes to it and form several new words like\n",
        "“JUMPS”, “JUMPED”, and “JUMPING”. In this case, the base word is “JUMP” and this is\n",
        "the word stem. If we were to carry out stemming on any of its three inflected forms, we\n",
        "would get the base form."
      ],
      "metadata": {
        "id": "Mu-IlUXhi5U_"
      },
      "id": "Mu-IlUXhi5U_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![2.1.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4RDaRXhpZgAATU0AKgAAAAgABAE7AAIAAAAFAAAISodpAAQAAAABAAAIUJydAAEAAAAKAAAQyOocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGRlbGwAAAAFkAMAAgAAABQAABCekAQAAgAAABQAABCykpEAAgAAAAM0OQAAkpIAAgAAAAM0OQAA6hwABwAACAwAAAiSAAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMzowODoyNSAxMjo0Njo0MQAyMDIzOjA4OjI1IDEyOjQ2OjQxAAAAZABlAGwAbAAAAP/hCxdodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIzLTA4LTI1VDEyOjQ2OjQxLjQ5MjwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5kZWxsPC9yZGY6bGk+PC9yZGY6U2VxPg0KCQkJPC9kYzpjcmVhdG9yPjwvcmRmOkRlc2NyaXB0aW9uPjwvcmRmOlJERj48L3g6eG1wbWV0YT4NCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgPD94cGFja2V0IGVuZD0ndyc/Pv/bAEMABwUFBgUEBwYFBggHBwgKEQsKCQkKFQ8QDBEYFRoZGBUYFxseJyEbHSUdFxgiLiIlKCkrLCsaIC8zLyoyJyorKv/bAEMBBwgICgkKFAsLFCocGBwqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKv/AABEIARcCIwMBIgACEQEDEQH/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/2gAMAwEAAhEDEQA/APpGiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKztf1yy8N6FdavqrtHaWqb5WVdxAzjp+NAGjRXmo+PXgggEXF6QehFm/+FL/AML58E/8973/AMA5P8KAPSaK82/4Xz4J/wCe97/4Byf4Uf8AC+fBP/Pe9/8AAOT/AAoA9Jorzb/hfPgn/nve/wDgHJ/hSf8AC+vBP/Pxe/8AgG/+FAHpVFZPhjxNpvi7QYtX0WRpLSYsEZlKkkHB4Na1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXDfGj/AJI74h/69v8A2YV3NcN8aP8AkjviH/r2/wDZhQBteEbK1bwZo7NbQkmyiyTGP7grY+w2n/PrD/37FZvg/wD5ErRv+vKL/wBAFbNAEH2G0/59Yf8Av2KPsNp/z6w/9+xU9IrK33SD9DQBD9htP+fWH/v2KqapZWq6PeEW0IIt3wfLH901pVU1b/kC3v8A17yf+gmgDz39nv8A5Izpn/XSX/0M16bXmX7Pf/JGdM/66S/+hmvTaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApks0cETSTyLHGoyzOcAfjT68G8X3GpfFj4rP4KsLyez0HTV3ag8LYLt6UAehX3xk8Aadcvb3Xia0EiHDBNz4P1ANVv8Ahefw5/6GaD/v1J/8TVzRvhF4H0SBY7fw9ZzFRgvcxiRj781q/wDCB+Ev+hZ0n/wCj/woA8X8bfGs6X4h/tvwb4t0/U7DaqvpE0LqeOpBwOtN8SfHTw745+Eet6fMTp2ry2wVbZ8kSNuHCn8O9dH41+D9/wCKfEP2DTbXRNF8O7VLTW1mguHPcZA4qv4x+E/hfwJ8GNfbS7FZrwWwzeXADSZ3DkHt+FAHqvg//kStG/68ov8A0AVs1jeD/wDkStG/68ov/QBWzQB5T8edR1Ww0DSltbi7s9HmvAuq3lnnzIouPToDzWR4J0CG28WaZqvwz8Wy6xozZTU7W8vDIQMcMFOCDXafEnVPEWjR2F5pGkJrOkhyup2YQPIYz0Kg9a810e1i8QfFzRdV+Hfhq90G1tix1SeWHyInXH3Nvc0AfQdVNW/5At7/ANe8n/oJq3VTVv8AkC3v/XvJ/wCgmgDz39nv/kjOmf8AXSX/ANDNem15l+z3/wAkZ0z/AK6S/wDoZr02gAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooARuFP0rxD4JRiT4ieNbqQs0puyu4ntXt7/AHG+leJ/A/8A5Hjxr/1/GgD22iiigArhvjR/yR3xD/17f+zCu5rhvjR/yR3xD/17f+zCgDoPB/8AyJWjf9eUX/oArZrivCvjXwxb+ENJhn1/Tkkjs4lZWuVBBCjjrWt/wnfhT/oYtM/8Ck/xoA36RVVfuqB9BWD/AMJ34U/6GLTP/ApP8aP+E78Kf9DFpn/gUn+NAG/VTVv+QLe/9e8n/oJrL/4Tvwp/0MWmf+BSf41W1Pxx4Wk0m7RPEOmlmgcAC5Xk7T70Ac1+z3/yRnTP+ukv/oZr02vMv2ev+SM6Z/10l/8AQzXptABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAj/cb6V4n8D/+R48af9fpr2x/uN9K8T+B/wDyPHjT/r9NAHttFFFABVXU9Ms9Z02bT9Tt0ubWddskTjIYe9WqKAOO/wCFSeBP+hYsP+/dH/CpPAn/AELFh/37rsaKAOO/4VJ4E/6Fiw/790f8Kk8Cf9CxYf8AfuuxooA47/hUngT/AKFiw/790f8ACpPAn/QsWP8A37rsaKAKOj6Lp2gaamn6PaR2lpGSVijGAM8mr1FFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUyaaO3iaWeRYo1GWd2wB9SafXF/F//kkfiH/r0P8AMUAdL/b2kf8AQUsv/AhP8aP7d0j/AKCll/4EJ/jXl/hf4GeAdT8J6XfXekSvPcWsckjfa5RlioJOA1av/DP3w6/6A0v/AIGS/wDxVAHd/wBu6R/0FLL/AMCE/wAaP7d0j/oKWX/gQn+NcJ/wz98Ov+gNL/4GS/8AxVH/AAz98Ov+gNL/AOBkv/xVAHd/27pH/QUsv/AhP8aP7d0j/oKWX/gQn+NcJ/wz/wDDr/oDS/8AgZL/APFVHP8AAL4dx20rro0uVQkf6ZL6f71AHf8A9u6R/wBBSy/8CE/xo/t3SP8AoKWX/gQn+NeFfB/4S+DvFvw+h1TXNNknu2nlRnFzImQrYHAIFd1/wz98Ov8AoDS/+Bkv/wAVQB3f9u6R/wBBSy/8CE/xo/t3SP8AoKWX/gQn+NcJ/wAM/fDr/oDS/wDgZL/8VR/wz/8ADr/oDS/+Bkv/AMVQB3f9u6R/0FLL/wACE/xo/t3SP+gpZf8AgQn+NcJ/wz/8Ov8AoDS/+Bkv/wAVR/wz/wDDr/oDS/8AgZL/APFUAd3/AG7pH/QUsv8AwIT/ABo/t3SP+gpZf+BCf414L8OPhR4P8Q+IvGFpqumySw6XqXkWqi5kXYmM44PP1Nd//wAM/fDr/oDS/wDgZL/8VQB3f9u6R/0FLL/wIT/Gj+3dI/6Cll/4EJ/jXCf8M/8Aw6/6A0v/AIGS/wDxVH/DP3w6/wCgNL/4GS//ABVAHd/27pH/AEFLL/wIT/Gj+3dI/wCgpZf+BCf41wn/AAz/APDr/oDS/wDgZL/8VR/wz/8ADr/oDS/+Bkv/AMVQB3f9u6R/0FLL/wACE/xo/t3SP+gpZf8AgQn+NeFQ/CXwc/xxuPDjaZJ/ZiaULhYvtMmRJuxndnPTtmu6/wCGf/h1/wBAaX/wMl/+KoA7v+3dI/6Cll/4EJ/jR/bukf8AQUsv/AhP8a4T/hn74df9AaX/AMDJf/iqP+Gf/h1/0Bpf/AyX/wCKoA7v+3dI/wCgpZf+BCf40f27pH/QUsv/AAIT/GuE/wCGf/h1/wBAaX/wMl/+Ko/4Z/8Ah1/0Bpf/AAMl/wDiqAO9j1nTJpVih1G0kkY4VFnUkn2Gau14L4m+HfhrwP8AEnwI/huxe1e61B1lLTu+4KoI+8T6171QAUUUUAI/3G+leJ/A/wD5Hjxp/wBfpr2uQ7YnJ6BSa8U+Age78ReMdQRcQvqDID6kUAe20UUUAFMmnit4WluJEijUZZ3YAD6k0+uG+NH/ACR3xD/17D/0IUAdV/bukf8AQUsv/AhP8aP7d0j/AKCll/4EJ/jXmXh34FeANQ8M6beXWkSNNPaxySN9rlGWKgn+KtL/AIZ/+HX/AEBpf/AyX/4qgDu/7d0j/oKWX/gQn+NH9u6R/wBBSy/8CE/xrhP+Gfvh1/0Bpf8AwMl/+Ko/4Z/+HX/QGl/8DJf/AIqgDu/7d0j/AKCll/4EJ/jR/bukf9BSy/8AAhP8a4T/AIZ/+HX/AEBpf/AyX/4qq9/8BPh7b6bczR6NKHjhdlP2yXqAT/eoA9D/ALd0j/oKWX/gQn+NH9u6R/0FLL/wIT/GvDPg/wDCTwd4t+Gtjq+u6bJcXszyB5BcyJnDEDgEDpXcf8M/fDr/AKA0v/gZL/8AFUAd3/bukf8AQUsv/AhP8aP7d0j/AKCll/4EJ/jXCf8ADP3w6/6A0v8A4GS//FUf8M//AA6/6A0v/gZL/wDFUAd3/bukf9BSy/8AAhP8aP7d0j/oKWX/AIEJ/jXCf8M/fDr/AKA0v/gZL/8AFUf8M/8Aw6/6A0v/AIGS/wDxVAHodre2t6rNZ3MNwqnDGKQMAfwqevIPgLp9tpVx41sbFDHbW2smKJSxbaoXgZNev0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXF/F/8A5JH4h/69D/MV2lcX8X/+SR+If+vQ/wAxQBr+CP8AkQ9D/wCvGL/0AVu1heCP+RD0P/rxi/8AQRW7QAUUVwdv4r1OT44XXhlnT+zotOW4Vdvzbz70Ad5UN3/x5T/9c2/lU1Q3f/HlP/1zb+VAHmv7PX/JJrf/AK+p/wD0OvUK8v8A2e/+STW//X1P/wCh16hQAUV5x8WvGWr+ErjwwujyIg1HVI7efegbKEgED061N8Y/F+qeDPCdpf6K6JPLfRQsXXcNrHmgD0Gio7dzLaxO3VkBP5VJQB5R8Hf+Rw+IX/YYH/oJr1evKPg7/wAjh8Qv+wwP/QTXq9ABRXnHxw8aat4F8Cw6roTxpcNeJETIgYFSGJH6Vt+HPHdjrPwyi8WySKkC2rS3B7KyD5x+YNAHWUV5F8DPiTrXxCl159aMfl2sifZ1RNu1WzwfXoK9doA8utv+TnLr/sBj/wBDFeo15dbf8nOXX/YDH/oYr1GgAoorwzQfEvxP8Za94gTQNT0yC10rUHt1S4gySATgZHsKAPc6K4L4Y+OtQ8Wx6rp+v2cVrq2j3Jt7gQtlH9GFd7QB5B8adWstD8Y+AtR1OYQWtvfyvLIR90bV5ra/4Xx8PP8AoPR/98NWb8W7WC98ffD63u4I54ZNQlDxyqGVhtHUGu7/AOEK8Lf9C7pX/gHH/hQBy3/C+Ph5/wBB6P8A74amv8e/h6kZYa4rY/hWM5NdX/whXhb/AKF3Sv8AwDj/AMKB4L8Lggjw9pYI/wCnOP8AwoA8o8TfGW98ZWb6B8MdFv7u5vFMTX0sZjSEHqR68d8ivQ/hj4IXwH4Pi055POu5W866l/vSHrXUWljZ2Efl2VtDbp/diQKP0qxmgAoozRmgArhvjR/yR3xD/wBe3/swql4z+KF94H8SeXqvhq6k0DC51SH5lUnrke1UviH4u0Lxf8D/ABDdeHtShvUFqCwRvmT5h95eooA7rwf/AMiVo3/XlF/6AK2axvB//IlaN/15Rf8AoArZoAKK8y+Mni7xD4Y/4Ry28LTQw3Oq3rW7NMm4dBj9TUuhad8VYtctH13WdJm04PmeOGDDMvsaAPSKqat/yBb3/r3k/wDQTVuqmrf8gW9/695P/QTQB57+z1/yRnTP+ukv/oZr02vMv2e/+SM6Z/10l/8AQzXptABRXmPxn8Y6/wCFLfQYvDE0MNxqV8LZmmTcOcAfqaz38b+N/Aer6ZD8QYbG90nUZhAdRtAU+zuegYe9AHr1FIrBlDLyCMg0tAHlPwW/5DXjz/sOt/6DXq1eU/BX/kNePP8AsOt/6DXq1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVxfxf/5JH4h/69D/ADFdpXF/F/8A5JH4h/69D/MUAa/gj/kQ9D/68Yv/AEEVu1heCP8AkQ9D/wCvGL/0EVu0Acn4t+IGn+Ddc0ey1eN47fVHZBeE4jiYdmPvmuL8O6ja69+0jq1/o8q3lpb6YkMk8XKK/pmvUdX0PTNes/sus2EF7BnISdAwB9RnpUeieG9H8OWzQaFptvYxscsIUC7vqe9AGnUN3/x5T/8AXNv5VNUN3/x5T/8AXNv5UAea/s9/8kmt/wDr6n/9Dr1CvL/2e/8Akk1v/wBfU/8A6HXqFAHjv7QcTR2fhXU3VvsljrEUtzIFyIkyPmPtxWf8bvE+ieKvDOjaT4d1K31G/u9SheGC3bcxUHk8dOvevbLuztr+1ktr2CO4gkGHilUMrD3BrG0rwJ4X0S9N3pWhWNtcE5EiQjcv0Pb8KANq2Qx2kKNwyoAfyqWiigDyj4O/8jh8Qv8AsMD/ANBNer15R8Hf+Rw+IX/YYH/oJr1egDyP9owI3gPS1lUMjaxbhlPQj5q841PT9W0fWLv4Q6akps9Zv4ruC4Bx5ds/Mgx6Dn8q+ltU0fTtat0g1azhvIo5BIqTLuAYdD9aG0fTn1WLU3soGvoY/LjuCg3qvoD6UAeRfA6yttN8dePLGyUJBbXUUaKPQBhXtVUbHRdN0y6urnT7KG3nu233EkaYMp9T61eoA8utv+TnLr/sBj/0MV6jXl1t/wAnOXX/AGAx/wChivUaAA9K+cPh34R1jxN4j8ZvpPi++0KOLV5FeK1RWEhJPJz09K+j6o6doum6TJcPpllDatdSGWcxJjzG9T6mgDF8DeA9O8C6bNBYyS3VzdSGW6u5zmSZz3NdRRRQB478cNHh1/xX4F0y5lmhiub6VGkgfa6javQ9qtf8M+aD/wBB7xB/4HmqXx0XVX8T+Bl8PtGuom+l+zmX7obavWofsHxy/wCf3TaANP8A4Z80H/oPeIP/AAPNH/DPmg/9B7xB/wCB5rM+wfHL/n902kNl8c1GRd6axH8OetAGp/wz5oP/AEHvEH/geaP+GfNB/wCg94g/8DzWNJ8UvH/gSaE/Ebw5C2mucNfWbFtn1xxXsmj6tZ67pNvqWmyia2uEDow9KAPNP+GfNB/6D3iD/wADzR/wz5oP/Qe8Qf8Agea9YooA+XPH3wxuI9ZPhjwjp/iPVLhlVnu7u9P2dQfY9cUuo/AU+CfhXres6vq00moLbBvs1s22IHcOG/vda+osDOcc+tcN8aP+SO+If+vb/wBmFAHQeD/+RK0b/ryi/wDQBWzWN4P/AORK0b/ryi/9AFbNAHiP7R0UE8ngyK8uWtYH1NhJMjbTGu0ZIPatrwbo3g/SPE1vcaZ44vNTumBSO1uNQEiuSP7vc16FrPh3SPEMUUWuadb36RMWjWdAwUnqRWfY+APCem30V5YeH7C3uYTujljhAZT6g0AdFVTVv+QLe/8AXvJ/6Cat1U1b/kC3v/XvJ/6CaAPPf2ev+SM6Z/10l/8AQzXpteZfs9f8kZ0z/rpL/wChmvTaAPFv2hpo4JvBcs7rHGmsIzuxwFAK5NQfGHxPpXjSx0bwj4WvI9T1K61GKUrbHcIkTqSeg6/pXr+seHtI8Qwxxa3p1vfRxNuRZ0DBT6iodI8J+H9BkMmjaNZWUhGC8MKqx/HrQBp28ZhtYoySSiBST3wKkoooA8p+Cv8AyGvHn/Ydb/0GvVq8p+C3/Ia8ef8AYdb/ANBr1agAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK4v4v8A/JI/EP8A16H+YrtKyfFPh+HxV4XvtEuZ5LeK9iMbSRgFlHtmgCDwR/yIeh/9eMX/AKAK3a8lh+BlxbwpDB8QfEkccahURZVAUDoBxT/+FI3n/RRPE3/f5f8ACgD1eivKP+FI3n/RRPE3/f5f8KP+FI3n/RRPE3/f5f8ACgD1eorv/jyn/wCubfyry3/hSN5/0UXxN/3+X/CkPwRuyCD8RPExB6/vl/woAsfs9/8AJJrf/r6n/wDQ69QryCx+AX9mWottO8deIbWAEkRwuiqCepwBVj/hSN5/0UTxN/3+X/CgD1eivKP+FI3n/RRfE3/f5f8ACj/hSN5/0UTxN/3+X/CgD1eivKP+FI3n/RRfE3/f5f8ACj/hSN5/0UTxN/3+X/CgA+Do/wCKv+IX/YZH/oJr1evH7X4ACxlnksvHXiCB7l98zRuimRvVuOTVn/hSN5/0UTxN/wB/l/woA9Xoryj/AIUjef8ARRPE3/f5f8KP+FI3n/RRfE3/AH+X/CgD1eivKP8AhSN5/wBFF8Tf9/l/wo/4Ujef9FE8Tf8Af5f8KAJrYH/hpy7/AOwGP/QxXqNeQD4BY1A348d+IRdlPLM+9N5X+7nHSrH/AApG8/6KJ4m/7/L/AIUAer0V5R/wpG8/6KJ4m/7/AC/4Uf8ACkbz/oonib/v8v8AhQB6vRXlH/Ckbz/oonib/v8AL/hR/wAKRvP+iieJv+/y/wCFAEvxTH/Fxfh3/wBhGX/0EV6lXl2m/BNbPxHpur33i/WtTfTZvOhiu2Vlz+XFeo0AFFFFAFDW9Ktdb0O706/jEtvcRlHUivJv2d72W3sPEHh6Z2ZNN1BlhDHO1PSvZ3+430rxP4H/API8eNP+v00Ae20UUUAFcN8aP+SO+If+vb/2YV3NY3i3w5D4u8K32hXU8lvFex7GliALLyDxnjtQAeEP+RK0b/ryi/8AQBWzXksXwMuIIUih+IXiRI0UKqrKoAA7Din/APCkbz/oonib/v8AL/hQB6vRXlH/AApG8/6KJ4m/7/L/AIUf8KRvP+iieJv+/wAv+FAHq9VNV/5A17/17yf+gmvM/wDhSN5/0UXxN/3+X/Ckb4H3TqVb4h+JirDBBmXkflQBb/Z7/wCSM6Z/10l/9DNem15BY/AL+zLRbXTvHfiG1t0ztihdFUZ9gKsf8KRvP+iieJv+/wAv+FAHq9FeUf8ACkbz/oonib/v8v8AhR/wpG8/6KL4m/7/AC/4UAer0V5R/wAKRvP+iieJv+/y/wCFH/Ckbz/oovib/v8AL/hQAvwW/wCQ148/7Drf+g16tXH/AA++Htv8P7XUYoNTu9SfULgXEst1t3bsY7da7CgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACisv8A4SPSxrF1pbXSi7tIPtE0Z42x/wB76VVTxtoEmkWWppqEbWl9N5FvIP43yRj9DQBvUVy8PxI8K3GurpMWrwtdNIYk5+RnH8IboT7VDf8AxQ8JaZrE+mXuqLHdW7hJVKHCH3PagDrqK5vWfH3hzQZbWPUdQVWu4vOg2KX3p/eGO1a+laxYa5p6Xuk3cV1bv0eNsj6exoAu0Vy178SPCun6ydMutVjSdWCO2CUjY/ws3QH2pNY+JHhnQb97PU75oZVKjPlsVJYZGDjBzQB1VFcfL8UvCkMVu8l+ym5LeUhhbc23qcYzU9v8R/DNx9mAv/LN1cfZoRKhTfJ6c0AdTRWVrfibSfDrWi6veJbteTCGAN1dz2FatABRRRQAUUUUAFFFFABRRRQAUUUUAI/3G+leJ/A//kePGn/X6a9sf7jfSvE/gf8A8jx40/6/TQB7bRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeF/GqW98N+NbLV9PXP9uWL6M+B/E7Ag/gK5K20O/ttab4cIryJ4eW41SGbnL7ohtH/fRavpPUtF07WDbHU7OK5NrKJoPMXPluOjD3oTRNNTW5NYSziGoSxCF7jb8zIOgz6UAeBXtxobfsx6Naac1r/a5lt1iiXHni584buPvZ6/hUUFp4jml+I39lrprCNl+2fbI9z58r5gp7d/xr2+DwH4XtteOtQaJZpqBbd54jGd3r9a0INA0u2kv5ILKJG1E5uyF/1xxj5vXigDxPwvc6XH4s+HUjSxx2f/COuqtduo5z0JPHWus+EpgbxL43n0pf+JRJqYNs0Y/ds235ynbGfSuqvPhx4R1C1tbe90K1mhs1KQIy8RqTnA9s1vadptnpNjHZ6bbR2ttEMJFEuFWgD5/1BrXS9D8SXukXenav4bmvpX1LSNRURXSS5AYI3X0I/Suv+JIs5Phz4Rls7bybd9R04xRyjLImVwpJ5yBgV2998PvCmpaz/a19oVnNe5DGZo+SR3PrWtqGj6fqtvDBqFrHPFBIssaOOFZfukfSgDx74k/bIfjl4Z/sa703T7j+zZsS36AwgbueMjmsf4maXqfiRPCmnSapp91q5nndLjTQBH5ijcgwCeeK9p1/wT4d8UzRTa/pNvfSQqVjaVclR6VHpXgLwxojQtpej21sYJTNGUX7jkYJH4UAfPfifxHd+PdT8O+JZomis9O1G205YpOMztkyHHsUH519SL90fSsZ/B3h+S1S3bSrfyY7r7YqBMATf3/rzW1QAUUUUAFFFFABRRRQAUUUUAFFFFACP9xvpXifwP8A+R48af8AX8a9sf7jfSvE/gf/AMjx41H/AE+mgD22iiigArkvijq99oXwy1rUtJuDbXlvBuilUAlTuA711tcN8aP+SO+If+vb/wBmFAHNaT4U+Jeq6NZ36/EZkF1CkoU2SHbuGcdKt/8ACC/E3/opLf8AgCld14P/AORK0b/ryi/9AFbNAHln/CC/E3/opLf+AKUf8IL8Tf8AopLf+AKV6nRQB5Z/wgvxN/6KS3/gClRXXgv4mW1pNOfiQxEUbOR9hTnAz6V6xVTVv+QLe/8AXvJ/6CaAPE/ANp8SfHXg+216Lx+9otwzr5RtEbbtOOuK6T/hBfib/wBFJb/wBSpv2ev+SM6Z/wBdJf8A0M16bQB5Z/wgvxN/6KS3/gClH/CC/E3/AKKS3/gClep0UAeWf8IL8Tf+ikt/4ApR/wAIL8Tf+ikt/wCAKV6nRQB4J4Oh+JHi6812CPx89udIvjZkm0Q+ZgZ3dOK6j/hBfib/ANFJb/wBSmfBX/kM+PP+w63/AKDXq1AHln/CC/E3/opLf+AKUf8ACC/E3/opLf8AgClep0UAeWf8IL8Tf+ikt/4ApR/wgvxN/wCikt/4ApXqdFAHj/g/UPF2mfGSbwx4j8SNrFulkJ+YVQZIPoPavYK8lt/+Torr/sFJ/Jq9aoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAA8jmvA/CF3F4D/aC13SdXP2aDWf31rJIdqk/U175XHfED4a6P8QLGNb/AHW17D/qLyIfPHQB2IORkciivFIfh/8AFvQofJ0fxxbXdunCJcxHdj3JqT+wPjf/ANDHpv8A37/+tQB6FqHxB8N6V4oGganqUVpfMoZVmO0NnpyayfjJIkvwZ8QPE6ujWwIZTkH5hXgXxWtfE8MiWfjLV9Kv9SIBjhggzP7YIGRUen6N8TLT4T65Jq00ltoAtQTBe8u43DhO4oA+o/B//IlaN/15Rf8AoArZrG8H/wDIlaN/15Rf+gCtmgCjrWpLo+h3mouhkW1haUoDgtgZxVLwd4lj8X+E7HXIIGgjvE3iNjkrzimeOP8AkQ9a/wCvOT/0E1gfBP8A5I7oH/XA/wAzQB3tVNW/5At7/wBe8n/oJq3VTVv+QLe/9e8n/oJoA89/Z6/5Izpn/XSX/wBDNem15l+z1/yRnTP+ukv/AKGa9NoA4n4kfEmz+G9rp1zf2klxFe3Hkko2PLGMlverfinx7Y+HfBEXiaGP7daTNEE8ph8wc8HNef8A7RdrDff8IfaXS74J9WWORfVTgH+dcN40vrjwX4a1T4d6y8jwi8hudEnYZDw7+U/CgD6gtphc2sU4GBIgcD0yM1JVXS/+QRaf9cE/9BFWqAPKfgt/yGfHn/Ydb/0GvVq8p+C3/Ia8ef8AYdb/ANBr1agClrGr2Wg6Pc6nqkywWtsheR2PQD+teYJ8eFRYdQvvCWsWnh+Zwq6rJEfL2k4DdOlaXx9z/wAKkvs7vL86Lzcf3dwzn2rO8e+ItU0f4eW0mneGNP1zw0NNje6aecqAu0cBR17UAeq2d3Bf2cN3ZyrNBMgeORDkMp6EVNXP+A9Qj1XwDo19BZx2Mc9qjrbRHKxD+6PaugoA8lt/+Torr/sFJ/Jq9aryW3/5Oiuv+wUn8mr1qgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDLm8MaJc60NXuNLtZdQChRcPGC4A6cmua+M/Hwc8Q4/wCfb/2YV3NcN8aP+SO+If8Ar2/9mFAHQeD/APkStG/68ov/AEAVs1jeD/8AkStG/wCvKL/0AVs0AU9Y02PWNGu9OmdkjuomiZl6gEYyKpeEvDdv4Q8LWWh2c0k8NomxZJMbm5zzitmigAqpq3/IFvf+veT/ANBNW6qat/yBb3/r3k/9BNAHnv7PX/JGdM/66S/+hmvTa8y/Z7/5Izpn/XSX/wBDNem0Acp428A2fjeXSHvbqa3Ol3Yuo/Kx85GODntxUPj/AOGuj/EKzs49UaSCeylEkNxCBvA7rz2OBXY0UAR28It7aKFSSI0CAnvgYqSiigDyn4K/8hrx5/2HW/8AQa9Wryn4Lf8AIa8ef9h1v/Qa9WoAqapplnrOl3GnalAs9rcIUkjYcEGvLW+AUDINPbxdrZ0INn+yzKNm3Odmf7teu0UAVdM0210fS7bTtOiENrbRiOKMfwqKtUUUAeS2/wDydFdf9gpP5NXrVeS2/wDydFdf9gpP5NXrVABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXL/EnQr3xL8OtX0jS1Rru6h2RB22gncD1/CuoooA8i024+MumaXa2MOg+HmjtoliUtO+SFGOeatf2t8af+gB4c/7/AL//ABVep0UAeWf2t8af+gB4c/7/AL//ABVH9rfGn/oAeHP+/wC//wAVXqdFAHlf9r/Gn/oX/Dv/AH/f/wCKplxqPxouLaWF9A8OhZEKHE79CMf3q9XooA8O8GaT8XfBHhe30LTNG0Oa3gZirzztuO455wQK3v7W+NP/AEAPDn/f9/8A4qvU6KAPK/7X+NP/AEAPDn/f9/8A4ql/tb40/wDQA8Of9/3/APiq9TooA8s/tb40/wDQA8Of9/3/APiqP7X+NP8A0L/hz/v+/wD8VXqdFAHhfhfRPi34UutXnsNG0OVtWuzdzCWdsIxGMLgjiuh/tf40/wDQA8Of9/3/APiq9TooA8s/tb40/wDQA8Of9/3/APiqT+1vjT/0APDn/f8Af/4qvVKKAPLP7W+NP/QA8Of9/wB//iqP7X+NP/QA8O/9/wB//iq9TooA8m8H+GvG03xXl8VeMLHT7VXsxb4s5Sw4Bxwee9es0UUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXDfEjxJqfh+68NJpcwiW+1RLefKg7kIORXc1xvj/wnfeJ7nw/JYPEg03UUupfMPVQO3vQByMHxP1Ow+MWtaDrBH9j/AOps7jYAsM+3cFZvfn8a5XS/if4i1C28PjVvEsOkxXst2Li7eJcfu3wg6eld/P8ADCTVr7xkurtE1prbxy2hQndDIi4VvYg1g+DPg5qei3HhxdbNjeW+l/afPQjcH8xsqQDQBZm8Wavp+reEorDxPFrdnrGpPFLOka4KKh+UY963PjD4n1LwxpGjyaXqCacLvUUt57l0DBIyDk803x94H1S+vvDd54Nt7CFtGunuPIl/do2Vx2qj4t8MeN/GGg6d9utdJiv9P1FLpIlkZopFUdDkUAZ+o+NdR0bw5cXOjeK4fEV7e3ENhaDygEt5ZGwHbHXg9K2F0r4k22r22mT69Hc2d1Fvl1BIAGtpF/hA7hs/pUOp+D/FXivRJ7PV7PSNIuLaWO70+exYkeejZG8Y5HFbWh6X43vfFNvqnim/trSztIGjWwsHJS4c9XfI9uBQBw+k+IvGcngLxH4mufEXmtpEl3AkHkKA5jJCsf51pv4x8T63/wAIb4e0a8htNQ1jT/t97fyR7tiKuSFXpkmtKx+Huq23ww8UeHnlg+16tcXUsDAnaBI2VzUUvw+1/TovCur6DPanWtCsvsc0M5PlXEZXDDPb60Acr4l+I3i/wrpPiHQbm+hu9c0qW3ktr2OEATRSnhWX+92rWl+KuqarP4Pl0hGgkvpJ4dQsZFG/zY487PUc/wA6brHwo8ReILHV9U1S5sv7e1S5tzsiyIoIYiCFB7n3q94l+Ed3f/E7T/FOi3qWYCn7WozlZNuBIo6Z6Z+lAGT4V8Y+LdXuNPuo9fs7jUJLrbqGgXEYheCPODszySOvvXt1eQ3Pw+8X65f6Mmu/2SJdLuknbWoci5uFU/dIAGMjrXr1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB//9k=)"
      ],
      "metadata": {
        "id": "lDM06kxdjNVc"
      },
      "id": "lDM06kxdjNVc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming helps us standardize\n",
        "words to their base stem irrespective of their inflections, which helps many applications\n",
        "like classifying or clustering text or even in information retrieval. Search engines use\n",
        "such techniques extensively to give better accurate results irrespective of the word form.\n",
        "The NLTK package has several implementations for stemmers. These stemmers are\n",
        "implemented in the stem module, which inherits the StemmerI interface in the nltk.\n",
        "stem.api module.\n",
        "\n",
        "One of the most popular stemmers is the Porter\n",
        "stemmer, which is based on the algorithm developed by its inventor, Martin Porter.\n",
        "Originally, the algorithm is said to have a total of five different phases for reduction of\n",
        "inflections to their stems, where each phase has its own set of rules. There also exists a\n",
        "Porter2 algorithm, which was the original stemming algorithm with some improvements\n",
        "suggested by Dr. Martin Porter."
      ],
      "metadata": {
        "id": "IviUzALwjjJL"
      },
      "id": "IviUzALwjjJL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ],
      "metadata": {
        "id": "T0aT56S2N-sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1fbb43e-8214-400b-a795-cde668ec25cd"
      },
      "id": "T0aT56S2N-sv",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('lying')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "762R8_2y7-Yg",
        "outputId": "523793f9-aca7-456a-b6c7-56ca3ec7bce7"
      },
      "id": "762R8_2y7-Yg",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('strange')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5D3mdhr38A3s",
        "outputId": "38f04846-d7a0-4b12-a7ee-85a1ab7b7596"
      },
      "id": "5D3mdhr38A3s",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'strang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "he Lancaster stemmer is based on the Lancaster stemming algorithm, also\n",
        "often known as the Paice/Husk stemmer, which was invented by Chris D. Paice."
      ],
      "metadata": {
        "id": "X4ySunf48LUn"
      },
      "id": "X4ySunf48LUn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancaster Stemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2seoHdUi8EUx",
        "outputId": "3306c762-b326-49b6-f626-841919b787de"
      },
      "id": "2seoHdUi8EUx",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls.stem('lying')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eSghdjuN8Vsn",
        "outputId": "46a298eb-2967-4181-ab8a-63314e96bf59"
      },
      "id": "eSghdjuN8Vsn",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lying'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls.stem('strange')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uFn1rKTS8YPR",
        "outputId": "960e0365-140d-4d77-a29a-cc56fed1771d"
      },
      "id": "uFn1rKTS8YPR",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'strange'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Lemmatization***\n",
        "The process of lemmatization is very similar to stemming, where we remove word affixes\n",
        "to get to a base form of the word. However in this case, this base form is also known\n",
        "as the root word but not the root stem. The difference between the two is that the root\n",
        "stem may not always be a lexicographically correct word, i.e., it may not be present in\n",
        "the dictionary but the root word, also known as the lemma, will always be present in the\n",
        "dictionary.\n",
        "\n",
        "The lemmatization process is considerably slower than stemming because an\n",
        "additional step is involved where the root form or lemma is formed by removing the affix\n",
        "from the word if and only if the lemma is present in the dictionary. The NLTK package\n",
        "has a robust lemmatization module where it uses WordNet and the word’s syntax and\n",
        "semantics like part of speech and context to get the root word or lemma.\n"
      ],
      "metadata": {
        "id": "eiRlcgQl8lS_"
      },
      "id": "eiRlcgQl8lS_"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize nouns\n",
        "print(wnl.lemmatize('cars', 'n'))\n",
        "print(wnl.lemmatize('men', 'n'))\n",
        "\n",
        "# lemmatize verbs\n",
        "print(wnl.lemmatize('running', 'v'))\n",
        "print(wnl.lemmatize('ate', 'v'))\n",
        "\n",
        "# lemmatize adjectives\n",
        "print(wnl.lemmatize('saddest', 'a'))\n",
        "print(wnl.lemmatize('fancier', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLeVI9668Zjk",
        "outputId": "68d15af3-43f1-451f-e2b5-bde3c6f0d3c2"
      },
      "id": "bLeVI9668Zjk",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n",
            "men\n",
            "run\n",
            "eat\n",
            "sad\n",
            "fancy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(words):\n",
        "     from nltk.stem import WordNetLemmatizer\n",
        "     from nltk.tokenize import word_tokenize\n",
        "     lemmatizer = WordNetLemmatizer()\n",
        "     a = []\n",
        "     tokens = word_tokenize(words)\n",
        "     for token in tokens:\n",
        "          lemmetized_word = lemmatizer.lemmatize(token)\n",
        "          a.append(lemmetized_word)\n",
        "     sentence = \" \".join(a)\n",
        "     return sentence"
      ],
      "metadata": {
        "id": "HDFDvN2iAeMH"
      },
      "id": "HDFDvN2iAeMH",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Students' studies are for making teachers cry. Teachers' cries are for NLTK sentence example.\""
      ],
      "metadata": {
        "id": "rDeD-RSPBxz3"
      },
      "id": "rDeD-RSPBxz3",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "O4dJBWv1CAmm",
        "outputId": "9c1ffed7-527b-4d63-eba0-b3cd8591e0dc"
      },
      "id": "O4dJBWv1CAmm",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Students ' study are for making teacher cry . Teachers ' cry are for NLTK sentence example .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Removing Stopwords***\n",
        "Stopwords are words that have little or no significance and are usually removed from\n",
        "text when processing it so as to retain words having maximum significance and context.\n",
        "Stopwords usually occur most frequently if you aggregate a corpus of text based on\n",
        "singular tokens and checked their frequencies. Words like “a,” “the,” “and,” and so on are\n",
        "stopwords. There is no universal or exhaustive list of stopwords and often each domain\n",
        "or language has its own set of stopwords. We depict a method to filter out and remove\n",
        "stopwords for English in the following code snippet\n"
      ],
      "metadata": {
        "id": "TT9S6Si199zf"
      },
      "id": "TT9S6Si199zf"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "nltk.download('stopwords')\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  if is_lower_case:\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  else:\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHh6H5i_8_NA",
        "outputId": "bd62da7e-b934-4f99-a321-3c094944e770"
      },
      "id": "OHh6H5i_8_NA",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "70SNFjJ_-e5K",
        "outputId": "6d958d92-9626-433c-8c4d-7e498f7ac653"
      },
      "id": "70SNFjJ_-e5K",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', , stopwords , computer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Bringing It All Together — Building a Text Normalizer***\n",
        "\n",
        "\n",
        "Let’s now bring everything we learned together and chain these operations to build a text\n",
        "normalizer to preprocess text data. We focus on including the major components often\n",
        "used for text wrangling in our custom function"
      ],
      "metadata": {
        "id": "xuU01jIU-ny_"
      },
      "id": "xuU01jIU-ny_"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "accented_char_removal=True, text_lower_case=True,\n",
        "text_lemmatization=True, special_char_removal=True,\n",
        "stopword_removal=True, remove_digits=True):\n",
        "  normalized_corpus = []\n",
        "  # normalize each document in the corpus\n",
        "  for doc in corpus:\n",
        "    # strip HTML\n",
        "    if html_stripping:\n",
        "      doc = strip_html_tags(doc)\n",
        "    # remove accented characters\n",
        "    if accented_char_removal:\n",
        "      doc = remove_accented_chars(doc)\n",
        "    # expand contractions\n",
        "    if contraction_expansion:\n",
        "      doc = expand_contractions(doc)\n",
        "    # lowercase the text\n",
        "    if text_lower_case:\n",
        "      doc = doc.lower()\n",
        "    # remove extra newlines\n",
        "    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "    # lemmatize text\n",
        "    if text_lemmatization:\n",
        "      doc = lemmatize_text(doc)\n",
        "    # remove special characters and\\or digits\n",
        "    if special_char_removal:\n",
        "      # insert spaces between special characters to isolate them\n",
        "      special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "      doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "      doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
        "    # remove extra whitespace\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    # remove stopwords\n",
        "    if stopword_removal:\n",
        "      doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "\n",
        "    normalized_corpus.append(doc)\n",
        "  return normalized_corpus"
      ],
      "metadata": {
        "id": "UgDQnPFT-iyf"
      },
      "id": "UgDQnPFT-iyf",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vinR_n75_Mx3",
        "outputId": "21cfa6c7-aba6-4e3f-d59f-1358e2e59f2b"
      },
      "id": "vinR_n75_Mx3",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_corpus([sample_text])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JVrYc85I_UHb",
        "outputId": "4fe199f7-49a2-4fb9-9366-1b789b02017b"
      },
      "id": "JVrYc85I_UHb",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u unveils world powerful supercomputer beat china u ha unveiled world powerful supercomputer called summit beating previous recordholder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit ha server reportedly take size two tennis court'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hul0seHj_bq3"
      },
      "id": "Hul0seHj_bq3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}