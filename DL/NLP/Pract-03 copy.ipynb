{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**TF / IDF**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDF** - log(D/Di) <br/>\n",
    "**D** -> No. of documents <br/>\n",
    "**Di** -> Word contains in how many documents <br/> <br/>\n",
    "\n",
    "\n",
    "**TF** - Freq of that particular word / Total No. of words\n",
    "\n",
    "**Query array must be multiplied to find documents**\n",
    "\n",
    "## **Python Lib**\n",
    "### **Word Embedding Technique** <br/>\n",
    "**TF / IDF Vectorizer** <br/>\n",
    "**Count Vectorizer** <br/>\n",
    "**Glove** <br/>\n",
    "**FastTex** <br/>\n",
    "**BERT** <br/>\n",
    "**GPT** <br/>\n",
    "**One-Hot Encoder** <br/>\n",
    "**Word2Vec** <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"In the 21st century, digital technology has transformed the way we approach education, making online learning platforms more popular than ever before.\"\n",
    "str2 = \"The stock market experienced a significant downturn last week, leading to concerns about the overall health of the global economy.\"\n",
    "str3 = \"The Olympic Games are a showcase of human athleticism and international unity, bringing together athletes from all corners of the world to compete on a grand stage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1_list = word_tokenize(str1)\n",
    "str2_list = word_tokenize(str2)\n",
    "str3_list = word_tokenize(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', '21st', 'century', ',', 'digital', 'technology', 'has', 'transformed', 'the', 'way', 'we', 'approach', 'education', ',', 'making', 'online', 'learning', 'platforms', 'more', 'popular', 'than', 'ever', 'before', '.']\n",
      "['The', 'stock', 'market', 'experienced', 'a', 'significant', 'downturn', 'last', 'week', ',', 'leading', 'to', 'concerns', 'about', 'the', 'overall', 'health', 'of', 'the', 'global', 'economy', '.']\n",
      "['The', 'Olympic', 'Games', 'are', 'a', 'showcase', 'of', 'human', 'athleticism', 'and', 'international', 'unity', ',', 'bringing', 'together', 'athletes', 'from', 'all', 'corners', 'of', 'the', 'world', 'to', 'compete', 'on', 'a', 'grand', 'stage', '.']\n"
     ]
    }
   ],
   "source": [
    "print(str1_list)\n",
    "print(str2_list)\n",
    "print(str3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'the',\n",
       " '21st',\n",
       " 'century',\n",
       " ',',\n",
       " 'digital',\n",
       " 'technology',\n",
       " 'has',\n",
       " 'transformed',\n",
       " 'the',\n",
       " 'way',\n",
       " 'we',\n",
       " 'approach',\n",
       " 'education',\n",
       " ',',\n",
       " 'making',\n",
       " 'online',\n",
       " 'learning',\n",
       " 'platforms',\n",
       " 'more',\n",
       " 'popular',\n",
       " 'than',\n",
       " 'ever',\n",
       " 'before',\n",
       " '.',\n",
       " 'The',\n",
       " 'stock',\n",
       " 'market',\n",
       " 'experienced',\n",
       " 'a',\n",
       " 'significant',\n",
       " 'downturn',\n",
       " 'last',\n",
       " 'week',\n",
       " ',',\n",
       " 'leading',\n",
       " 'to',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'overall',\n",
       " 'health',\n",
       " 'of',\n",
       " 'the',\n",
       " 'global',\n",
       " 'economy',\n",
       " '.',\n",
       " 'The',\n",
       " 'Olympic',\n",
       " 'Games',\n",
       " 'are',\n",
       " 'a',\n",
       " 'showcase',\n",
       " 'of',\n",
       " 'human',\n",
       " 'athleticism',\n",
       " 'and',\n",
       " 'international',\n",
       " 'unity',\n",
       " ',',\n",
       " 'bringing',\n",
       " 'together',\n",
       " 'athletes',\n",
       " 'from',\n",
       " 'all',\n",
       " 'corners',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'to',\n",
       " 'compete',\n",
       " 'on',\n",
       " 'a',\n",
       " 'grand',\n",
       " 'stage',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "li += str1_list\n",
    "li += str2_list\n",
    "li += str3_list\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we', 'approach', 'leading', 'grand', 'the', 'global', 'stage', 'popular', 'all', 'Olympic', 'and', 'compete', 'transformed', 'of', 'on', 'making', 'ever', 'The', 'market', 'international', 'significant', 'concerns', 'to', 'platforms', 'together', 'century', 'economy', 'more', '21st', 'than', 'athleticism', 'showcase', 'week', 'corners', 'experienced', 'last', 'world', 'from', 'a', 'before', 'downturn', ',', 'digital', 'learning', 'stock', 'about', 'Games', 'athletes', 'human', 'are', 'technology', 'health', 'has', 'education', 'way', 'In', 'online', '.', 'bringing', 'overall', 'unity'}\n"
     ]
    }
   ],
   "source": [
    "li = set(li)\n",
    "print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['we', 1, 0, 0], ['approach', 1, 0, 0], ['leading', 0, 1, 0], ['grand', 0, 0, 1], ['the', 2, 2, 1], ['global', 0, 1, 0], ['stage', 0, 0, 1], ['popular', 1, 0, 0], ['all', 0, 0, 1], ['Olympic', 0, 0, 1], ['and', 0, 0, 1], ['compete', 0, 0, 1], ['transformed', 1, 0, 0], ['of', 0, 1, 2], ['on', 0, 0, 1], ['making', 1, 0, 0], ['ever', 1, 0, 0], ['The', 0, 1, 1], ['market', 0, 1, 0], ['international', 0, 0, 1], ['significant', 0, 1, 0], ['concerns', 0, 1, 0], ['to', 0, 1, 1], ['platforms', 1, 0, 0], ['together', 0, 0, 1], ['century', 1, 0, 0], ['economy', 0, 1, 0], ['more', 1, 0, 0], ['21st', 1, 0, 0], ['than', 1, 0, 0], ['athleticism', 0, 0, 1], ['showcase', 0, 0, 1], ['week', 0, 1, 0], ['corners', 0, 0, 1], ['experienced', 0, 1, 0], ['last', 0, 1, 0], ['world', 0, 0, 1], ['from', 0, 0, 1], ['a', 0, 1, 2], ['before', 1, 0, 0], ['downturn', 0, 1, 0], [',', 2, 1, 1], ['digital', 1, 0, 0], ['learning', 1, 0, 0], ['stock', 0, 1, 0], ['about', 0, 1, 0], ['Games', 0, 0, 1], ['athletes', 0, 0, 1], ['human', 0, 0, 1], ['are', 0, 0, 1], ['technology', 1, 0, 0], ['health', 0, 1, 0], ['has', 1, 0, 0], ['education', 1, 0, 0], ['way', 1, 0, 0], ['In', 1, 0, 0], ['online', 1, 0, 0], ['.', 1, 1, 1], ['bringing', 0, 0, 1], ['overall', 0, 1, 0], ['unity', 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "mat = []\n",
    "i = 0\n",
    "for tot_token in li:\n",
    "    counter = 0\n",
    "    mat.append([tot_token, 0, 0, 0])\n",
    "    if tot_token in str1_list:\n",
    "        for token in str1_list:\n",
    "            if (token == tot_token):\n",
    "                counter += 1\n",
    "        mat[i][1] = counter\n",
    "        counter = 0\n",
    "    if tot_token in str2_list:\n",
    "        for token in str2_list:\n",
    "            if (token == tot_token):\n",
    "                counter += 1\n",
    "        mat[i][2] = counter\n",
    "        counter = 0\n",
    "    if tot_token in str3_list:\n",
    "        for token in str3_list:\n",
    "            if (token == tot_token):\n",
    "                counter += 1\n",
    "        mat[i][3] = counter\n",
    "        counter = 0\n",
    "    i += 1\n",
    "\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['we', '1', '0', '0'],\n",
       "       ['approach', '1', '0', '0'],\n",
       "       ['leading', '0', '1', '0'],\n",
       "       ['grand', '0', '0', '1'],\n",
       "       ['the', '2', '2', '1'],\n",
       "       ['global', '0', '1', '0'],\n",
       "       ['stage', '0', '0', '1'],\n",
       "       ['popular', '1', '0', '0'],\n",
       "       ['all', '0', '0', '1'],\n",
       "       ['Olympic', '0', '0', '1'],\n",
       "       ['and', '0', '0', '1'],\n",
       "       ['compete', '0', '0', '1'],\n",
       "       ['transformed', '1', '0', '0'],\n",
       "       ['of', '0', '1', '2'],\n",
       "       ['on', '0', '0', '1'],\n",
       "       ['making', '1', '0', '0'],\n",
       "       ['ever', '1', '0', '0'],\n",
       "       ['The', '0', '1', '1'],\n",
       "       ['market', '0', '1', '0'],\n",
       "       ['international', '0', '0', '1'],\n",
       "       ['significant', '0', '1', '0'],\n",
       "       ['concerns', '0', '1', '0'],\n",
       "       ['to', '0', '1', '1'],\n",
       "       ['platforms', '1', '0', '0'],\n",
       "       ['together', '0', '0', '1'],\n",
       "       ['century', '1', '0', '0'],\n",
       "       ['economy', '0', '1', '0'],\n",
       "       ['more', '1', '0', '0'],\n",
       "       ['21st', '1', '0', '0'],\n",
       "       ['than', '1', '0', '0'],\n",
       "       ['athleticism', '0', '0', '1'],\n",
       "       ['showcase', '0', '0', '1'],\n",
       "       ['week', '0', '1', '0'],\n",
       "       ['corners', '0', '0', '1'],\n",
       "       ['experienced', '0', '1', '0'],\n",
       "       ['last', '0', '1', '0'],\n",
       "       ['world', '0', '0', '1'],\n",
       "       ['from', '0', '0', '1'],\n",
       "       ['a', '0', '1', '2'],\n",
       "       ['before', '1', '0', '0'],\n",
       "       ['downturn', '0', '1', '0'],\n",
       "       [',', '2', '1', '1'],\n",
       "       ['digital', '1', '0', '0'],\n",
       "       ['learning', '1', '0', '0'],\n",
       "       ['stock', '0', '1', '0'],\n",
       "       ['about', '0', '1', '0'],\n",
       "       ['Games', '0', '0', '1'],\n",
       "       ['athletes', '0', '0', '1'],\n",
       "       ['human', '0', '0', '1'],\n",
       "       ['are', '0', '0', '1'],\n",
       "       ['technology', '1', '0', '0'],\n",
       "       ['health', '0', '1', '0'],\n",
       "       ['has', '1', '0', '0'],\n",
       "       ['education', '1', '0', '0'],\n",
       "       ['way', '1', '0', '0'],\n",
       "       ['In', '1', '0', '0'],\n",
       "       ['online', '1', '0', '0'],\n",
       "       ['.', '1', '1', '1'],\n",
       "       ['bringing', '0', '0', '1'],\n",
       "       ['overall', '0', '1', '0'],\n",
       "       ['unity', '0', '0', '1']], dtype='<U13')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = np.array(mat)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.         0.         0.        ]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.17609126 0.17609126 0.17609126]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.17609126 0.17609126 0.17609126]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.17609126 0.17609126 0.17609126]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.17609126 0.17609126 0.17609126]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.         0.         0.        ]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.         0.         0.        ]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]\n",
      " [0.47712125 0.47712125 0.47712125]]\n"
     ]
    }
   ],
   "source": [
    "tot_doc = 3\n",
    "log_sheet = []\n",
    "for item in tf:\n",
    "    contain_doc = 0\n",
    "    for i in range(1, 4):\n",
    "        if int(item[i]) > 0:\n",
    "            contain_doc += 1\n",
    "    data = np.log10(tot_doc/contain_doc)\n",
    "    log_sheet.append([data, data, data])\n",
    "    \n",
    "log_sheet =np.array(log_sheet)\n",
    "print(log_sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['2', '2', '1'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '1', '2'],\n",
       "       ['0', '0', '1'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '1', '1'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '1', '1'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '1', '2'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['2', '1', '1'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['1', '0', '0'],\n",
       "       ['0', '1', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '0', '0'],\n",
       "       ['1', '1', '1'],\n",
       "       ['0', '0', '1'],\n",
       "       ['0', '1', '0'],\n",
       "       ['0', '0', '1']], dtype='<U13')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IDF\n",
    "tf = np.delete(tf, 0, 1)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docdiv1 = len(str1)\n",
    "docdiv2 = len(str2)\n",
    "docdiv3 = len(str3)\n",
    "\n",
    "for item in tf:\n",
    "    item[0] = int(item[0])/docdiv1\n",
    "    item[1] = int(item[1])/docdiv2\n",
    "    item[2] = int(item[2])/docdiv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "61\n",
      "[['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.01333333333' '0.01538461538' '0.00609756097']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.00769230769' '0.01219512195']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.00769230769' '0.00609756097']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.00769230769' '0.00609756097']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.00769230769' '0.01219512195']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.01333333333' '0.00769230769' '0.00609756097']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.0' '0.0']\n",
      " ['0.00666666666' '0.00769230769' '0.00609756097']\n",
      " ['0.0' '0.0' '0.00609756097']\n",
      " ['0.0' '0.00769230769' '0.0']\n",
      " ['0.0' '0.0' '0.00609756097']]\n"
     ]
    }
   ],
   "source": [
    "print(len(log_sheet))\n",
    "print(len(tf))\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = tf.astype('float64') * log_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.00135455, 0.00214745],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.00135455, 0.00107373],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.00135455, 0.00107373],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.00135455, 0.00214745],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.00318081, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.00290928],\n",
       "       [0.        , 0.00367016, 0.        ],\n",
       "       [0.        , 0.        , 0.00290928]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-intel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
